{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Message Sentiment Classification\n",
    "### BOW - TF-IDF - NB\n",
    "---\n",
    "\n",
    "The goal of this notebook is to develop a machine learning model to analyze the sentiment of user messages. <br> \n",
    "The program will train this model from pre-labeled Twitter data, and the model will be used to assess sentiment of user message data referencing specific brands or products over time. <br>\n",
    "This notebook uses TF-IDF tranformed, bag of words corpus trained to a Naive Bayes classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Training Data \n",
    "\n",
    "Labeled Twitter training and test sentiment data: \n",
    "- http://help.sentiment140.com/for-students/\n",
    "- http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import analysis libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query_string</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>502054</th>\n",
       "      <td>0</td>\n",
       "      <td>2187397522</td>\n",
       "      <td>Mon Jun 15 20:13:21 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>jessica515</td>\n",
       "      <td>@xoxmichelle lmao to our discoveries! wish our...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219802</th>\n",
       "      <td>0</td>\n",
       "      <td>1976480205</td>\n",
       "      <td>Sat May 30 17:26:16 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>jlpnut</td>\n",
       "      <td>analyzing=good. overanalyzing=not so good.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1482848</th>\n",
       "      <td>4</td>\n",
       "      <td>2067353287</td>\n",
       "      <td>Sun Jun 07 12:11:43 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>jennamay0711</td>\n",
       "      <td>is watching the littlest vampireee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67736</th>\n",
       "      <td>0</td>\n",
       "      <td>1692489538</td>\n",
       "      <td>Sun May 03 20:06:22 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>xreeshix</td>\n",
       "      <td>@sXe_rockstar you're near a puter?  why are yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056121</th>\n",
       "      <td>4</td>\n",
       "      <td>1962415930</td>\n",
       "      <td>Fri May 29 11:00:43 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>StudioKiSun</td>\n",
       "      <td>@lyneL @_Gavia_ You gals are on a roll this mo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment          id                          date query_string  \\\n",
       "502054           0  2187397522  Mon Jun 15 20:13:21 PDT 2009     NO_QUERY   \n",
       "219802           0  1976480205  Sat May 30 17:26:16 PDT 2009     NO_QUERY   \n",
       "1482848          4  2067353287  Sun Jun 07 12:11:43 PDT 2009     NO_QUERY   \n",
       "67736            0  1692489538  Sun May 03 20:06:22 PDT 2009     NO_QUERY   \n",
       "1056121          4  1962415930  Fri May 29 11:00:43 PDT 2009     NO_QUERY   \n",
       "\n",
       "                 user                                               text  \n",
       "502054     jessica515  @xoxmichelle lmao to our discoveries! wish our...  \n",
       "219802         jlpnut        analyzing=good. overanalyzing=not so good.   \n",
       "1482848  jennamay0711                is watching the littlest vampireee   \n",
       "67736        xreeshix  @sXe_rockstar you're near a puter?  why are yo...  \n",
       "1056121   StudioKiSun  @lyneL @_Gavia_ You gals are on a roll this mo...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import training data csv\n",
    "cols = ['sentiment','id','date','query_string','user','text']\n",
    "df = pd.read_csv('../data/twitter_sentiment140/training.1600000.processed.noemoticon.csv', \n",
    "                 names=cols, encoding='latin1')\n",
    "\n",
    "# Restrict training data to sample of 100K records for code development\n",
    "df = df.sample(100000)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    50005\n",
       "1    49995\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Restrict to sentiment and text columns and remap sentiment values\n",
    "df = df[['sentiment','text']]\n",
    "df['sentiment'] = df.sentiment.map({0:0, 4:1})\n",
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Pre-Processing \n",
    "Since the training data is derived from Twitter posts, the text strings contain tags mentioning other users (using the @ symbol syntax) and website URLs to associate external content with each post. \n",
    "\n",
    "Cleaning logic will be implemented to apply the following to each text string: \n",
    "- Remove HTML tags\n",
    "- Remove @user mentions\n",
    "- Remove URL hyperlinks\n",
    "- Remove stop words\n",
    "- Remove punctuation\n",
    "- Lower case text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "atu_pattern    = r'@[A-Za-z0-9]+'\n",
    "url_pattern    = r'https?://[A-Za-z0-9./]+' \n",
    "remove_table   = str.maketrans({key: None for key in string.punctuation + string.digits})\n",
    "stopwords_list = [word.translate(remove_table) for word in stopwords.words('english')]\n",
    "\n",
    "def tweet_cleaner(text):   \n",
    "    # Remove html tags\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    \n",
    "    # Remove @user mentions and URLs\n",
    "    strip_1 = re.sub(atu_pattern, '', souped)\n",
    "    strip_2 = re.sub(url_pattern, '', strip_1)\n",
    "    \n",
    "    # Remove stop words, punctuation, and numbers\n",
    "    # Split words into list and lower case the text\n",
    "    clean_split = [word.lower().translate(remove_table) \n",
    "                   for word in strip_2.split() \n",
    "                   if word.lower().translate(remove_table) \n",
    "                       and word.lower().translate(remove_table) not in stopwords_list]\n",
    "    \n",
    "    return clean_split\n",
    "\n",
    "# Additional text processing can include stemming and lemmatization steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization and Transformation\n",
    "Create bag-of-words vectors for each cleaned text string and apply TF-IDF transformation to normalize word scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64354\n",
      "CPU times: user 24.8 s, sys: 1.27 s, total: 26.1 s\n",
      "Wall time: 26.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Fit BOW vector to tweet_cleaner applied corpus\n",
    "bow_transformer = CountVectorizer(analyzer=tweet_cleaner).fit(df['text'])\n",
    "\n",
    "# Print total number of vocab words\n",
    "print(len(bow_transformer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform text strings into BOW\n",
    "text_bow = bow_transformer.transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit TF-IDF transformer\n",
    "tfidf_transformer = TfidfTransformer().fit(text_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform BOW corpus into TF-IDF corpus\n",
    "text_tfidf = tfidf_transformer.transform(text_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training \n",
    "Train a Naive-Bayes model to assess sentiment of messages from the TF-IDF transformed BOW corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Naive-Bayes model to classify sentiment\n",
    "sent_model = MultinomialNB().fit(text_tfidf, df['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.86      0.84     50005\n",
      "           1       0.86      0.82      0.84     49995\n",
      "\n",
      "   micro avg       0.84      0.84      0.84    100000\n",
      "   macro avg       0.84      0.84      0.84    100000\n",
      "weighted avg       0.84      0.84      0.84    100000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test out the evaluation of the model using the training outcome labels\n",
    "pred = sent_model.predict(text_tfidf)\n",
    "print(classification_report(df['sentiment'], pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Pipeline\n",
    "Develop a workflow pipeline to apply the text cleaning, word vectorization, TF-IDF transformation, and classification training. The pipeine can then be used to more easily clean and generate predicitons of the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 359 entries, 0 to 497\n",
      "Data columns (total 2 columns):\n",
      "sentiment    359 non-null float64\n",
      "text         359 non-null object\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 8.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# Import test data csv\n",
    "cols = ['sentiment','id','date','query_string','user','text']\n",
    "test = pd.read_csv('../data/twitter_sentiment140/testdata.manual.2009.06.14.csv', names=cols, encoding='latin1')\n",
    "test = test[['sentiment','text']]\n",
    "test['sentiment'] = test.sentiment.map({0:0, 4:1})\n",
    "\n",
    "test = test.dropna()\n",
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.4 s, sys: 1.11 s, total: 25.5 s\n",
      "Wall time: 25.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Build workflow pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([('bow',        CountVectorizer(analyzer=tweet_cleaner)), \n",
    "                     ('tfidf',      TfidfTransformer()),  \n",
    "                     ('classifier', MultinomialNB())])\n",
    "\n",
    "# Fit pipeline with training data\n",
    "pipeline.fit(df.text, df.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.75      0.77       187\n",
      "           1       0.75      0.79      0.77       172\n",
      "\n",
      "   micro avg       0.77      0.77      0.77       359\n",
      "   macro avg       0.77      0.77      0.77       359\n",
      "weighted avg       0.77      0.77      0.77       359\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict test data with pipeline\n",
    "pred = pipeline.predict(test.text)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(pred, test.sentiment))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
